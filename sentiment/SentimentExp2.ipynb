{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ermFuE-ZNzJ3"
      },
      "source": [
        "Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8VSXZEcANyMT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-11 20:44:25.827856: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-04-11 20:44:25.912016: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-04-11 20:44:26.281940: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-11 20:44:27.224633: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer\n",
        "from einops import rearrange, repeat\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZoldsiwyNyv9"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"sst\", \"default\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3VHmBQfOczi",
        "outputId": "4b48cf36-3d39-49f2-ee9a-2db03e613c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8544\n",
            "1101\n"
          ]
        }
      ],
      "source": [
        "# longest sentence 267 -> max seqzence lengtha and input ? -> padding\n",
        "train_data = dataset[\"train\"][\"sentence\"]\n",
        "# labels are floats between 0 and 1, need to be rounded to zero or 1 for sentiment classification\n",
        "train_labels = dataset[\"train\"][\"label\"]\n",
        "print(len(train_data))\n",
        "\n",
        "val_data = dataset[\"validation\"][\"sentence\"]\n",
        "# labels are floats between 0 and 1, need to be rounded to zero or 1 for sentiment classification\n",
        "val_labels = dataset[\"validation\"][\"label\"]\n",
        "print(len(val_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c3Og45elOdja"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "vocab_size = tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MymfHdorbVJz"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing(data, targets, tokenizer, max_length):\n",
        "  tokenized_data = []\n",
        "  for input in data:\n",
        "      t = tokenizer.encode_plus(text=input, max_length=max_length, padding=\"max_length\")\n",
        "      tokenized_data.append(t[\"input_ids\"])\n",
        "  binary_targets = tf.math.round(targets)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((tokenized_data, binary_targets))\n",
        "  dataset = dataset.shuffle(1000).batch(32).prefetch(4)\n",
        "  return dataset\n",
        "\n",
        "train_dataset = data_preprocessing(train_data, train_labels, tokenizer, 267)\n",
        "validation_dataset = data_preprocessing(val_data, val_labels, tokenizer, 267)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wQlxOmqmfDt8"
      },
      "outputs": [],
      "source": [
        "class MambaResBlock(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.projection_dim = 2*input_dim\n",
        "\n",
        "    # normalisation\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    # Dense\n",
        "    self.dense1 = tf.keras.layers.Dense(units=self.projection_dim)\n",
        "    # Dense\n",
        "    self.dense2 = tf.keras.layers.Dense(units=self.projection_dim)\n",
        "    # Convolution\n",
        "    self.conv1d = tf.keras.layers.Conv1D(filters=self.projection_dim , kernel_size=4, strides=1 , padding=\"causal\", groups = self.projection_dim, data_format = \"channels_last\") # data_format?, groups?\n",
        "    # SSM block\n",
        "    self.ssm = SelectiveSSM(32, self.projection_dim)\n",
        "    # Dense\n",
        "    self.dense3 = tf.keras.layers.Dense(units=input_dim)\n",
        "    # dropout\n",
        "    self.dropout = tf.keras.layers.Dropout(rate=0.2)\n",
        "\n",
        "  # forward step\n",
        "  def call(self, input):\n",
        "\n",
        "    x = self.layernorm(input)\n",
        "\n",
        "    x1 = self.dense1(x)\n",
        "    x1 = self.conv1d(x1)\n",
        "    x1 =  tf.nn.silu(x1)\n",
        "    x1 = self.ssm(x1)\n",
        "\n",
        "    x2 = self.dense2(x)\n",
        "    x2 =  tf.nn.silu(x2)\n",
        "\n",
        "    x = x1 * x2\n",
        "    x = self.dense3(x)\n",
        "\n",
        "    # skip connection\n",
        "    x = x + input\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class SelectiveSSM(tf.keras.Model):\n",
        "  def __init__(self, states, projection_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.states = states\n",
        "    self.projection_dim = projection_dim\n",
        "\n",
        "    # hippo initialisation für A ? dafür müsste A aber quadratisch sein\n",
        "    # -> quadratische matrix oder nicht ?\n",
        "    #self.A =  # states x internal dim\n",
        "    #self.D =  # np ones internal dim\n",
        "    A = repeat(tf.range(1, states+1, dtype=tf.float32),'n -> d n', d=self.projection_dim)\n",
        "\n",
        "    # A parameter: matrix changing hidden state\n",
        "    self.A_log = tf.Variable(tf.math.log(A),trainable=True, dtype=tf.float32)\n",
        "\n",
        "    # D parameter: simple skip connection, initialisde as ones\n",
        "    self.D = tf.Variable(tf.ones(projection_dim),trainable=True, dtype=tf.float32) # change from np to tf\n",
        "\n",
        "    # B, C, delta parameter: dependend on input, therefore dense layer to learn these\n",
        "    self.denseB = tf.keras.layers.Dense(units=self.states)\n",
        "    self.denseC = tf.keras.layers.Dense(units=self.states)\n",
        "    self.densedelta = tf.keras.layers.Dense(units=self.projection_dim)\n",
        "\n",
        "  def selective_scan(self,input, delta, A, B, C, D):\n",
        "    \"\"\"\n",
        "    Calculate output of the selective state space model using parallel scan\n",
        "    implemented using the cumulative sum\n",
        "\n",
        "    args:\n",
        "      input: data input that we calculate the ssm on\n",
        "      delta: mediates how much focus is put on new input\n",
        "      A: state matrix controlling the hidden state\n",
        "      B: modulate the recurrent dynamics based on content (input)\n",
        "      C: modulate the recurrent dynamics based on context (hidden states)\n",
        "      D: scales the skip connection\n",
        "\n",
        "    returns:\n",
        "      output: result of the ssm with current parameters\n",
        "\n",
        "    \"\"\"\n",
        "    # first step of discretization of A\n",
        "    deltaA = tf.einsum('bld,dn->bldn', delta, A) # quasi delta mal A\n",
        "    deltaBinput = tf.einsum('bld,bld,bln->bldn', delta, input, B) # input mal B mal delta\n",
        "\n",
        "    deltaA_cumsum = tf.pad(\n",
        "        deltaA[:, 1:], [[0, 0], [1, 1], [0, 0], [0, 0]])[:, 1:, :, :]\n",
        "\n",
        "    deltaA_cumsum = tf.reverse(deltaA_cumsum, axis=[1])  # Flip along axis 1\n",
        "\n",
        "    # Cumulative sum along all the input tokens, parallel prefix sum,\n",
        "    # calculates dA for all the input tokens in parallel\n",
        "    deltaA_cumsum = tf.math.cumsum(deltaA_cumsum, axis=1)\n",
        "\n",
        "    # second step of discretization of A\n",
        "    deltaA_cumsum = tf.exp(deltaA_cumsum)\n",
        "    deltaA_cumsum = tf.reverse(deltaA_cumsum, axis=[1])  # Flip back along axis 1\n",
        "\n",
        "    # calculate intermediate output as in graphs shown for ssm's\n",
        "    x = deltaBinput * deltaA_cumsum\n",
        "    # 1e-12 to avoid division by 0\n",
        "    x = tf.math.cumsum(x, axis=1)/(deltaA_cumsum + 1e-12)\n",
        "\n",
        "    # intermediate output multiplied with parameter C\n",
        "    output = tf.einsum('bldn,bln->bld', x, C)\n",
        "\n",
        "    return output + input * D\n",
        "\n",
        "  def call(self, input):\n",
        "\n",
        "    A = -tf.exp(tf.cast(self.A_log, tf.float32))\n",
        "\n",
        "    C = self.denseC(input)\n",
        "    B = self.denseB(input)\n",
        "    # softplus to not get nan values\n",
        "    delta = tf.nn.softplus(self.densedelta(input))\n",
        "\n",
        "    return self.selective_scan(input, delta, A, B, C, self.D)\n",
        "\n",
        "class MambaModel(tf.keras.Model):\n",
        "  def __init__(self, num_layers, vocab_size, input_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim = vocab_size, output_dim = input_dim) #, input_length = 128) (bs, 128, 128)\n",
        "    self.layer_list = []\n",
        "    for i in range(num_layers):\n",
        "        self.layer_list.append(MambaResBlock(input_dim))\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.dense = tf.keras.layers.Dense(units=1024, activation=tf.nn.gelu)\n",
        "    self.out = tf.keras.layers.Dense(units=1, activation=tf.nn.sigmoid)\n",
        "\n",
        "  def call(self, input):\n",
        "\n",
        "    x = self.embedding(input)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.layer_list[i](x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense(x)\n",
        "    x = self.out(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AY8zDPvFhCDF"
      },
      "outputs": [],
      "source": [
        "model = MambaModel(3, vocab_size, input_dim=267)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "# compile the model (here, adding a loss function and an optimizer)\n",
        "model.compile(optimizer = optimizer, loss=loss, metrics=[\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset,validation_data=validation_dataset, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualise_results(history):\n",
        "\n",
        "  plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
        "  plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
        "  plt.title(\"Train and Validation Loss\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "  plt.plot(history.history[\"val_accuracy\"], label=\"validation accuracy\")\n",
        "  plt.title(\"Train and Validation Accuracy\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "visualise_results(history)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
