{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ermFuE-ZNzJ3"
      },
      "source": [
        "Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8VSXZEcANyMT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-11 11:20:36.101865: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-04-11 11:20:36.215831: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-04-11 11:20:36.653212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-11 11:20:37.791903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer\n",
        "from einops import rearrange, repeat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZoldsiwyNyv9"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"sst\", \"default\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3VHmBQfOczi",
        "outputId": "4b48cf36-3d39-49f2-ee9a-2db03e613c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8544\n",
            "1101\n"
          ]
        }
      ],
      "source": [
        "# longest sentence 267 -> max seqzence lengtha and input ? -> padding\n",
        "train_data = dataset[\"train\"][\"sentence\"]\n",
        "# labels are floats between 0 and 1, need to be rounded to zero or 1 for sentiment classification\n",
        "train_labels = dataset[\"train\"][\"label\"]\n",
        "print(len(train_data))\n",
        "\n",
        "val_data = dataset[\"validation\"][\"sentence\"]\n",
        "# labels are floats between 0 and 1, need to be rounded to zero or 1 for sentiment classification\n",
        "val_labels = dataset[\"validation\"][\"label\"]\n",
        "print(len(val_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c3Og45elOdja"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "vocab_size = tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MymfHdorbVJz"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing(data, targets, tokenizer, max_length):\n",
        "  tokenized_data = []\n",
        "  for input in data:\n",
        "      t = tokenizer.encode_plus(text=input, max_length=max_length, padding=\"max_length\")\n",
        "      tokenized_data.append(t[\"input_ids\"])\n",
        "  binary_targets = tf.math.round(targets)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((tokenized_data, binary_targets))\n",
        "  dataset = dataset.shuffle(1000).batch(32).prefetch(4)\n",
        "  return dataset\n",
        "\n",
        "train_dataset = data_preprocessing(train_data, train_labels, tokenizer, 267)\n",
        "validation_dataset = data_preprocessing(val_data, val_labels, tokenizer, 267)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wQlxOmqmfDt8"
      },
      "outputs": [],
      "source": [
        "class MambaResBlock(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.projection_dim = 2*input_dim\n",
        "\n",
        "    # normalisation\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    # Dense\n",
        "    self.dense1 = tf.keras.layers.Dense(units=self.projection_dim)\n",
        "    # Dense\n",
        "    self.dense2 = tf.keras.layers.Dense(units=self.projection_dim)\n",
        "    # Convolution\n",
        "    self.conv1d = tf.keras.layers.Conv1D(filters=self.projection_dim , kernel_size=4, strides=1 , padding=\"causal\", groups = self.projection_dim, data_format = \"channels_last\") # data_format?, groups?\n",
        "    # SSM block\n",
        "    self.ssm = SelectiveSSM(32, self.projection_dim)\n",
        "    # Dense\n",
        "    self.dense3 = tf.keras.layers.Dense(units=input_dim)\n",
        "    # dropout\n",
        "    self.dropout = tf.keras.layers.Dropout(rate=0.2)\n",
        "\n",
        "  # forward step\n",
        "  def call(self, input):\n",
        "\n",
        "    x = self.layernorm(input)\n",
        "\n",
        "    x1 = self.dense1(x)\n",
        "    x1 = self.conv1d(x1)\n",
        "    x1 =  tf.nn.silu(x1)\n",
        "    x1 = self.ssm(x1)\n",
        "\n",
        "    x2 = self.dense2(x)\n",
        "    x2 =  tf.nn.silu(x2)\n",
        "\n",
        "    x = x1 * x2\n",
        "    x = self.dense3(x)\n",
        "\n",
        "    # skip connection\n",
        "    x = x + input\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class SelectiveSSM(tf.keras.Model):\n",
        "  def __init__(self, states, projection_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.states = states\n",
        "    self.projection_dim = projection_dim\n",
        "\n",
        "    # hippo initialisation für A ? dafür müsste A aber quadratisch sein\n",
        "    # -> quadratische matrix oder nicht ?\n",
        "    #self.A =  # states x internal dim\n",
        "    #self.D =  # np ones internal dim\n",
        "    A = repeat(tf.range(1, states+1, dtype=tf.float32),'n -> d n', d=self.projection_dim)\n",
        "\n",
        "    # A parameter: matrix changing hidden state\n",
        "    self.A_log = tf.Variable(tf.math.log(A),trainable=True, dtype=tf.float32)\n",
        "\n",
        "    # D parameter: simple skip connection, initialisde as ones\n",
        "    self.D = tf.Variable(tf.ones(projection_dim),trainable=True, dtype=tf.float32) # change from np to tf\n",
        "\n",
        "    # B, C, delta parameter: dependend on input, therefore dense layer to learn these\n",
        "    self.denseB = tf.keras.layers.Dense(units=self.states)\n",
        "    self.denseC = tf.keras.layers.Dense(units=self.states)\n",
        "    self.densedelta = tf.keras.layers.Dense(units=self.projection_dim)\n",
        "\n",
        "  def selective_scan(self,input, delta, A, B, C, D):\n",
        "    \"\"\"\n",
        "    Calculate output of the selective state space model using parallel scan\n",
        "    implemented using the cumulative sum\n",
        "\n",
        "    args:\n",
        "      input: data input that we calculate the ssm on\n",
        "      delta: mediates how much focus is put on new input\n",
        "      A: state matrix controlling the hidden state\n",
        "      B: modulate the recurrent dynamics based on content (input)\n",
        "      C: modulate the recurrent dynamics based on context (hidden states)\n",
        "      D: scales the skip connection\n",
        "\n",
        "    returns:\n",
        "      output: result of the ssm with current parameters\n",
        "\n",
        "    \"\"\"\n",
        "    # first step of discretization of A\n",
        "    deltaA = tf.einsum('bld,dn->bldn', delta, A) # quasi delta mal A\n",
        "    deltaBinput = tf.einsum('bld,bld,bln->bldn', delta, input, B) # input mal B mal delta\n",
        "\n",
        "    deltaA_cumsum = tf.pad(\n",
        "        deltaA[:, 1:], [[0, 0], [1, 1], [0, 0], [0, 0]])[:, 1:, :, :]\n",
        "\n",
        "    deltaA_cumsum = tf.reverse(deltaA_cumsum, axis=[1])  # Flip along axis 1\n",
        "\n",
        "    # Cumulative sum along all the input tokens, parallel prefix sum,\n",
        "    # calculates dA for all the input tokens in parallel\n",
        "    deltaA_cumsum = tf.math.cumsum(deltaA_cumsum, axis=1)\n",
        "\n",
        "    # second step of discretization of A\n",
        "    deltaA_cumsum = tf.exp(deltaA_cumsum)\n",
        "    deltaA_cumsum = tf.reverse(deltaA_cumsum, axis=[1])  # Flip back along axis 1\n",
        "\n",
        "    # calculate intermediate output as in graphs shown for ssm's\n",
        "    x = deltaBinput * deltaA_cumsum\n",
        "    # 1e-12 to avoid division by 0\n",
        "    x = tf.math.cumsum(x, axis=1)/(deltaA_cumsum + 1e-12)\n",
        "\n",
        "    # intermediate output multiplied with parameter C\n",
        "    output = tf.einsum('bldn,bln->bld', x, C)\n",
        "\n",
        "    return output + input * D\n",
        "\n",
        "  def call(self, input):\n",
        "\n",
        "    A = -tf.exp(tf.cast(self.A_log, tf.float32))\n",
        "\n",
        "    C = self.denseC(input)\n",
        "    B = self.denseB(input)\n",
        "    # softplus to not get nan values\n",
        "    delta = tf.nn.softplus(self.densedelta(input))\n",
        "\n",
        "    return self.selective_scan(input, delta, A, B, C, self.D)\n",
        "\n",
        "class MambaModel(tf.keras.Model):\n",
        "  def __init__(self, num_layers, vocab_size, input_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim = vocab_size, output_dim = input_dim) #, input_length = 128) (bs, 128, 128)\n",
        "    self.layer_list = []\n",
        "    for i in range(num_layers):\n",
        "        self.layer_list.append(MambaResBlock(input_dim))\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.dense = tf.keras.layers.Dense(units=1024, activation=tf.nn.gelu)\n",
        "    self.out = tf.keras.layers.Dense(units=1, activation=tf.nn.sigmoid)\n",
        "\n",
        "  def call(self, input):\n",
        "\n",
        "    x = self.embedding(input)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.layer_list[i](x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense(x)\n",
        "    x = self.out(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AY8zDPvFhCDF"
      },
      "outputs": [],
      "source": [
        "model = MambaModel(3, vocab_size, input_dim=267)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "# compile the model (here, adding a loss function and an optimizer)\n",
        "model.compile(optimizer = optimizer, loss=loss, metrics=[\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "psbui3QIhTMm",
        "outputId": "e8a3f2bf-e245-426b-8f6d-c268a923d769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pbrstudent/anaconda3/envs/hsi/lib/python3.10/site-packages/keras/src/layers/layer.py:357: UserWarning: `build()` was called on layer 'mamba_model', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1712827289.625202   17595 service.cc:145] XLA service 0x72ce2a301000 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1712827289.625844   17595 service.cc:153]   StreamExecutor device (0): Host, Default Version\n",
            "I0000 00:00:1712827289.726520   17595 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "history = model.fit(train_dataset,validation_data=validation_dataset, epochs=8)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
